{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "129edc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv()  # 读取 .env 文件\n",
    "token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "login(token=token, new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb47f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74218203",
   "metadata": {},
   "source": [
    "# Part1 - Understanding Tokens in Large Language Models (5%)\n",
    "\n",
    "- Q1: What is the vocabulary size of the Gemma-3-1B tokenizer? (1%)\n",
    "- Q3: Encode the string「作業一」to token IDs (Gemma-3-1B). (1%)\n",
    "- Q4: Which pair correctly reports the longest decoded token string in the vocabulary (token_id, \n",
    "character_length)? (1%)\n",
    "- Q5: Given the prefix 「阿姆斯特朗旋風迴旋加速噴氣式阿姆斯特朗砲」 , which single Chinese \n",
    "character is the model’s most probable next token? (1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 262144\n",
      "Q3 [46306, 237009]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q1\", tokenizer.vocab_size)\n",
    "print(\"Q3\", tokenizer.encode(\"作業一\", add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c778b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4 (137, 31) -> '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "max_len, max_token_id = -1, None\n",
    "for i in range(tokenizer.vocab_size):\n",
    "    token = tokenizer.decode(i)\n",
    "    if len(token) > max_len:\n",
    "        max_len, max_token_id = len(token), i\n",
    "print(\"Q4\", (max_token_id, max_len), \"->\", repr(tokenizer.decode(max_token_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b6af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18, 262144])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'塔'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"阿姆斯特朗旋風迴旋加速噴氣式阿姆斯特朗砲\", return_tensors=\"pt\")\n",
    "outputs = model(input_ids)\n",
    "# logits: [batch_size, sequence_length, vocab_size]\n",
    "print(outputs.logits.shape)\n",
    "# 信心分数（可能为负数） --- softmax ---> 几率分布\n",
    "# P.S. 联想到 模拟退火中 Metropolis 接受概率函数\n",
    "probabilities = torch.softmax(outputs.logits[:, -1, :], dim=-1)\n",
    "# 首尾呼应、以终为始：LM Head 就是 Embedding Table\n",
    "tokenizer.decode(torch.argmax(probabilities.item(), dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b410c61",
   "metadata": {},
   "source": [
    "# Part2 - System and User Prompt Engineering (3%)\n",
    "\n",
    "- Q6: instruction following (1%)\n",
    "- Q7: restrictive system prompt (1%)\n",
    "- Q8: language constraint (1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ee1665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Q6-1: 皮卡丘的起源於 **《超時預言家》 (Time Traveler’s Tale)**。\n",
       "\n",
       "它最初是《超時預言家》的短篇故事，並於2008年首次播出。\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q6-2: Pikachu is derived from the **Pokémon** franchise! \n",
       "\n",
       "Specifically, he's based on the adorable and energetic Pokémon Pikachu, created by Satoshi Tajiri in 1996. 😊 \n",
       "\n",
       "Let me know if you’d like to learn more about the Pokémon franchise!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model_id)\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a smart agent.\"},\n",
    "    {\"role\": \"user\", \"content\": \"皮卡丘源自於哪個動畫作品?\"}]\n",
    "outputs = pipe(messages, max_new_tokens=2000, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "response = outputs[0][\"generated_text\"][-1]['content']\n",
    "display(Markdown('Q6-1: '+response))\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a smart agent.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Which anime is Pikachu derived from?\"}]\n",
    "outputs = pipe(messages, max_new_tokens=2000, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "response = outputs[0][\"generated_text\"][-1]['content']\n",
    "display(Markdown('Q6-2: '+response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2da0dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      " You can only answer: I don’t know.\n",
      "\n",
      "皮卡丘源自於哪個動畫作品?<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://ai.google.dev/gemma/docs/core/prompt-structure\n",
    "# Gemma 的指令调优模型仅适用于两个角色：user 和 model\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90cb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Q7: I don’t know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model_id)\n",
    "messages = [{\"role\": \"system\", \"content\": \"You can only answer: I don’t know.\"},\n",
    "    {\"role\": \"user\", \"content\": \"皮卡丘源自於哪個動畫作品?\"}]\n",
    "outputs = pipe(messages, max_new_tokens=2000, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "response = outputs[0][\"generated_text\"][-1]['content']\n",
    "display(Markdown('Q7: '+response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96963283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Q8: Pikachu originated from the anime series **Pokemon**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model_id)\n",
    "messages = [{\"role\": \"system\", \"content\": \"Answer in English only\"},\n",
    "    {\"role\": \"user\", \"content\": \"皮卡丘源自於哪個動畫作品?\"}]\n",
    "outputs = pipe(messages, max_new_tokens=2000, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "response = outputs[0][\"generated_text\"][-1]['content']\n",
    "display(Markdown('Q8: '+response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
