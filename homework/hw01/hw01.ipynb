{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "129edc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "load_dotenv()  # è¯»å– .env æ–‡ä»¶\n",
    "token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "login(token=token, new_session=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb47f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-3-1b-it\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74218203",
   "metadata": {},
   "source": [
    "# Part1 - Understanding Tokens in Large Language Models (5%)\n",
    "\n",
    "- Q1: What is the vocabulary size of the Gemma-3-1B tokenizer? (1%)\n",
    "- Q3: Encode the stringã€Œä½œæ¥­ä¸€ã€to token IDs (Gemma-3-1B). (1%)\n",
    "- Q4: Which pair correctly reports the longest decoded token string in the vocabulary (token_id, \n",
    "character_length)? (1%)\n",
    "- Q5: Given the prefix ã€Œé˜¿å§†æ–¯ç‰¹æœ—æ—‹é¢¨è¿´æ—‹åŠ é€Ÿå™´æ°£å¼é˜¿å§†æ–¯ç‰¹æœ—ç ²ã€ , which single Chinese \n",
    "character is the modelâ€™s most probable next token? (1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29b93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 262144\n",
      "Q3 [46306, 237009]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q1\", tokenizer.vocab_size)\n",
    "print(\"Q3\", tokenizer.encode(\"ä½œæ¥­ä¸€\", add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c778b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q4 (137, 31) -> '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "max_len, max_token_id = -1, None\n",
    "for i in range(tokenizer.vocab_size):\n",
    "    token = tokenizer.decode(i)\n",
    "    if len(token) > max_len:\n",
    "        max_len, max_token_id = len(token), i\n",
    "print(\"Q4\", (max_token_id, max_len), \"->\", repr(tokenizer.decode(max_token_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87b6af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18, 262144])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å¡”'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"é˜¿å§†æ–¯ç‰¹æœ—æ—‹é¢¨è¿´æ—‹åŠ é€Ÿå™´æ°£å¼é˜¿å§†æ–¯ç‰¹æœ—ç ²\", return_tensors=\"pt\")\n",
    "outputs = model(input_ids)\n",
    "# logits: [batch_size, sequence_length, vocab_size]\n",
    "print(outputs.logits.shape)\n",
    "# ä¿¡å¿ƒåˆ†æ•°ï¼ˆå¯èƒ½ä¸ºè´Ÿæ•°ï¼‰ --- softmax ---> å‡ ç‡åˆ†å¸ƒ\n",
    "# P.S. è”æƒ³åˆ° æ¨¡æ‹Ÿé€€ç«ä¸­ Metropolis æ¥å—æ¦‚ç‡å‡½æ•°\n",
    "probabilities = torch.softmax(outputs.logits[:, -1, :], dim=-1)\n",
    "# é¦–å°¾å‘¼åº”ã€ä»¥ç»ˆä¸ºå§‹ï¼šLM Head å°±æ˜¯ Embedding Table\n",
    "tokenizer.decode(torch.argmax(probabilities.item(), dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b410c61",
   "metadata": {},
   "source": [
    "# Part2 - System and User Prompt Engineering (3%)\n",
    "\n",
    "- Q6: instruction following (1%)\n",
    "- Q7: restrictive system prompt (1%)\n",
    "- Q8: language constraint (1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ee1665b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Q6-1: çš®å¡ä¸˜çš„èµ·æºæ–¼ **ã€Šè¶…æ™‚é è¨€å®¶ã€‹ (Time Travelerâ€™s Tale)**ã€‚\n",
       "\n",
       "å®ƒæœ€åˆæ˜¯ã€Šè¶…æ™‚é è¨€å®¶ã€‹çš„çŸ­ç¯‡æ•…äº‹ï¼Œä¸¦æ–¼2008å¹´é¦–æ¬¡æ’­å‡ºã€‚\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Q6-2: Pikachu is derived from the **PokÃ©mon** franchise! \n",
       "\n",
       "Specifically, he's based on the adorable and energetic PokÃ©mon Pikachu, created by Satoshi Tajiri in 1996. ğŸ˜Š \n",
       "\n",
       "Let me know if youâ€™d like to learn more about the PokÃ©mon franchise!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model_id)\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a smart agent.\"},\n",
    "    {\"role\": \"user\", \"content\": \"çš®å¡ä¸˜æºè‡ªæ–¼å“ªå€‹å‹•ç•«ä½œå“?\"}]\n",
    "outputs = pipe(messages, max_new_tokens=2000, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "response = outputs[0][\"generated_text\"][-1]['content']\n",
    "display(Markdown('Q6-1: '+response))\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a smart agent.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Which anime is Pikachu derived from?\"}]\n",
    "outputs = pipe(messages, max_new_tokens=2000, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "response = outputs[0][\"generated_text\"][-1]['content']\n",
    "display(Markdown('Q6-2: '+response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2da0dd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      " You can only answer: I donâ€™t know.\n",
      "\n",
      "çš®å¡ä¸˜æºè‡ªæ–¼å“ªå€‹å‹•ç•«ä½œå“?<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://ai.google.dev/gemma/docs/core/prompt-structure\n",
    "# Gemma çš„æŒ‡ä»¤è°ƒä¼˜æ¨¡å‹ä»…é€‚ç”¨äºä¸¤ä¸ªè§’è‰²ï¼šuser å’Œ model\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f90cb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Q7: I donâ€™t know."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model_id)\n",
    "messages = [{\"role\": \"system\", \"content\": \"You can only answer: I donâ€™t know.\"},\n",
    "    {\"role\": \"user\", \"content\": \"çš®å¡ä¸˜æºè‡ªæ–¼å“ªå€‹å‹•ç•«ä½œå“?\"}]\n",
    "outputs = pipe(messages, max_new_tokens=2000, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "response = outputs[0][\"generated_text\"][-1]['content']\n",
    "display(Markdown('Q7: '+response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96963283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Q8: Pikachu originated from the anime series **Pokemon**.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model_id)\n",
    "messages = [{\"role\": \"system\", \"content\": \"Answer in English only\"},\n",
    "    {\"role\": \"user\", \"content\": \"çš®å¡ä¸˜æºè‡ªæ–¼å“ªå€‹å‹•ç•«ä½œå“?\"}]\n",
    "outputs = pipe(messages, max_new_tokens=2000, pad_token_id=pipe.tokenizer.eos_token_id)\n",
    "response = outputs[0][\"generated_text\"][-1]['content']\n",
    "display(Markdown('Q8: '+response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
